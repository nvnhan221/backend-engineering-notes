{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Backend Engineering Notes","text":"<p>Welcome to the Backend Engineering Notes documentation!</p> <p>This documentation site is built using MkDocs Material, following the Docs-as-code approach.</p>"},{"location":"#what-is-docs-as-code","title":"What is Docs-as-code?","text":"<p>Docs-as-code is a documentation methodology that treats documentation like source code:</p> <ul> <li>\u2705 Version controlled with Git</li> <li>\u2705 Written in Markdown</li> <li>\u2705 Built and deployed automatically</li> <li>\u2705 Collaborative and reviewable</li> <li>\u2705 Easy to maintain and update</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Foundations - Core concepts and principles</li> <li>Messaging - Message queues and event-driven patterns</li> <li>Examples - Practical code examples</li> <li>Microservices - Microservices architecture</li> <li>Data - Database and data management</li> <li>Observability - Monitoring, logging, and tracing</li> <li>Reliability - Building resilient systems</li> <li>Security - Security best practices</li> <li>ADR - Architecture Decision Records</li> <li>Getting Started - Set up your environment</li> </ul>"},{"location":"#features","title":"Features","text":"<p>This documentation site includes:</p> <ul> <li>\ud83d\udcda Material Design - Beautiful, responsive UI</li> <li>\ud83d\udd0d Full-text Search - Find content quickly</li> <li>\ud83c\udf13 Dark Mode - Easy on the eyes</li> <li>\ud83d\udcf1 Mobile Friendly - Works on all devices</li> <li>\ud83d\udd17 Version Control - Track changes over time</li> <li>\ud83d\ude80 Static Site - Fast and reliable hosting</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please read our contributing guidelines and submit pull requests.</p> <p>Last updated: January 16, 2026</p>"},{"location":"adr/","title":"Architecture Decision Records (ADR)","text":"<p>Documenting important architectural decisions and their context.</p>"},{"location":"adr/#what-are-adrs","title":"What are ADRs?","text":"<p>Architecture Decision Records (ADRs) are documents that capture important architectural decisions made along with their context and consequences.</p>"},{"location":"adr/#adr-format","title":"ADR Format","text":"<p>Each ADR should include:</p> <ul> <li>Status - Proposed, Accepted, Rejected, Deprecated</li> <li>Context - The issue motivating this decision</li> <li>Decision - The change that we're proposing or have agreed to implement</li> <li>Consequences - What becomes easier or more difficult to do</li> </ul>"},{"location":"adr/#adr-template","title":"ADR Template","text":"<pre><code># ADR-001: [Title]\n\n## Status\n[Proposed | Accepted | Rejected | Deprecated]\n\n## Context\n[Describe the issue and context]\n\n## Decision\n[Describe the decision]\n\n## Consequences\n[Describe the positive and negative consequences]\n</code></pre>"},{"location":"adr/#current-adrs","title":"Current ADRs","text":"<p>No ADRs yet. Add your first ADR to start documenting decisions.</p> <p>Add ADRs as you make important architectural decisions</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete API reference documentation.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>This section contains detailed API documentation for all endpoints, methods, and data structures.</p>"},{"location":"api/#endpoints","title":"Endpoints","text":""},{"location":"api/#authentication","title":"Authentication","text":"<ul> <li><code>POST /api/auth/login</code> - User login</li> <li><code>POST /api/auth/logout</code> - User logout</li> <li><code>POST /api/auth/refresh</code> - Refresh access token</li> </ul>"},{"location":"api/#users","title":"Users","text":"<ul> <li><code>GET /api/users</code> - List all users</li> <li><code>GET /api/users/{id}</code> - Get user by ID</li> <li><code>POST /api/users</code> - Create new user</li> <li><code>PUT /api/users/{id}</code> - Update user</li> <li><code>DELETE /api/users/{id}</code> - Delete user</li> </ul>"},{"location":"api/#data-models","title":"Data Models","text":""},{"location":"api/#user","title":"User","text":"<pre><code>{\n  \"id\": \"string\",\n  \"email\": \"string\",\n  \"name\": \"string\",\n  \"createdAt\": \"datetime\",\n  \"updatedAt\": \"datetime\"\n}\n</code></pre>"},{"location":"api/#authentication_1","title":"Authentication","text":"<p>All API requests require authentication using Bearer tokens:</p> <pre><code>Authorization: Bearer &lt;your-token&gt;\n</code></pre>"},{"location":"api/#rate-limiting","title":"Rate Limiting","text":"<p>API requests are rate-limited to 100 requests per minute per IP address.</p>"},{"location":"api/#error-responses","title":"Error Responses","text":"<p>All errors follow this format:</p> <pre><code>{\n  \"error\": {\n    \"code\": \"ERROR_CODE\",\n    \"message\": \"Human-readable error message\",\n    \"details\": {}\n  }\n}\n</code></pre>"},{"location":"api/#status-codes","title":"Status Codes","text":"<ul> <li><code>200 OK</code> - Request successful</li> <li><code>201 Created</code> - Resource created</li> <li><code>400 Bad Request</code> - Invalid request</li> <li><code>401 Unauthorized</code> - Authentication required</li> <li><code>403 Forbidden</code> - Insufficient permissions</li> <li><code>404 Not Found</code> - Resource not found</li> <li><code>500 Internal Server Error</code> - Server error</li> </ul> <p>API documentation is automatically generated from code annotations.</p>"},{"location":"data/","title":"Data","text":"<p>Database design, data modeling, and data management patterns.</p>"},{"location":"data/#overview","title":"Overview","text":"<p>This section covers database design, data modeling, storage patterns, and data management best practices.</p>"},{"location":"data/#topics","title":"Topics","text":"<ul> <li>Database design patterns</li> <li>Data modeling techniques</li> <li>Data migration strategies</li> <li>Data consistency patterns</li> <li>Caching strategies</li> </ul>"},{"location":"data/#database-types","title":"Database Types","text":""},{"location":"data/#relational-databases","title":"Relational Databases","text":"<p>SQL databases for structured data with relationships.</p>"},{"location":"data/#nosql-databases","title":"NoSQL Databases","text":"<p>Document, key-value, column-family, and graph databases.</p>"},{"location":"data/#time-series-databases","title":"Time-Series Databases","text":"<p>Optimized for time-stamped data.</p>"},{"location":"data/#data-patterns","title":"Data Patterns","text":"<ul> <li>CQRS - Command Query Responsibility Segregation</li> <li>Event Sourcing - Store events instead of current state</li> <li>Sharding - Horizontal partitioning</li> <li>Replication - Data redundancy</li> </ul> <p>Add more data-related topics as needed</p>"},{"location":"examples/","title":"Examples","text":"<p>Code examples and snippets for common backend engineering tasks.</p>"},{"location":"examples/#available-examples","title":"Available Examples","text":""},{"location":"examples/#messaging-examples","title":"Messaging Examples","text":"<ul> <li>Kafka Java Examples - Complete Kafka examples with Java</li> <li>Kafka Outbox Pattern - Transactional outbox pattern</li> <li>RabbitMQ Retry DLX - Retry with Dead Letter Exchange</li> </ul>"},{"location":"examples/#api-examples","title":"API Examples","text":"<ul> <li>REST API Example - Complete REST API implementation</li> </ul>"},{"location":"examples/#example-structure","title":"Example Structure","text":"<p>Each example includes:</p> <ul> <li>\u2705 Complete, runnable code</li> <li>\u2705 Explanations and comments</li> <li>\u2705 Best practices</li> <li>\u2705 Common pitfalls to avoid</li> </ul>"},{"location":"examples/#running-examples","title":"Running Examples","text":"<p>Most examples can be run directly:</p> <pre><code># Navigate to the example directory\ncd examples/\n\n# Follow the README in each example folder\n</code></pre>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>Have a useful example? Share it!</p> <ol> <li>Create a new example file in <code>docs/examples/</code></li> <li>Add it to the navigation in <code>mkdocs.yml</code></li> <li>Include clear explanations and comments</li> <li>Test that the example works</li> <li>Submit a pull request</li> </ol> <p>More examples coming soon!</p>"},{"location":"examples/kafka-java/","title":"Kafka Java Examples","text":"<p>Complete Kafka examples using Java with the official Kafka client library.</p>"},{"location":"examples/kafka-java/#overview","title":"Overview","text":"<p>This guide provides complete, production-ready examples for using Apache Kafka with Java, including producers, consumers, and consumer groups.</p>"},{"location":"examples/kafka-java/#prerequisites","title":"Prerequisites","text":"<ul> <li>Java 8 or higher</li> <li>Maven 3.6+</li> <li>Kafka cluster running (local or remote)</li> </ul>"},{"location":"examples/kafka-java/#project-setup","title":"Project Setup","text":""},{"location":"examples/kafka-java/#maven-dependencies","title":"Maven Dependencies","text":"<p>Add to your <code>pom.xml</code>:</p> <pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;\n        &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;\n        &lt;version&gt;3.5.0&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.slf4j&lt;/groupId&gt;\n        &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt;\n        &lt;version&gt;2.0.7&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"examples/kafka-java/#gradle-dependencies","title":"Gradle Dependencies","text":"<p>Add to your <code>build.gradle</code>:</p> <pre><code>dependencies {\n    implementation 'org.apache.kafka:kafka-clients:3.5.0'\n    implementation 'org.slf4j:slf4j-simple:2.0.7'\n}\n</code></pre>"},{"location":"examples/kafka-java/#basic-producer","title":"Basic Producer","text":""},{"location":"examples/kafka-java/#simple-producer","title":"Simple Producer","text":"<pre><code>package com.example.kafka;\n\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.Producer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.common.serialization.StringSerializer;\n\nimport java.util.Properties;\n\npublic class SimpleProducer {\n    public static void main(String[] args) {\n        Properties props = new Properties();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n\n        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);\n\n        try {\n            for (int i = 0; i &lt; 10; i++) {\n                ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(\n                    \"my-topic\",\n                    \"key-\" + i,\n                    \"value-\" + i\n                );\n                producer.send(record);\n                System.out.println(\"Sent: \" + record);\n            }\n        } finally {\n            producer.close();\n        }\n    }\n}\n</code></pre>"},{"location":"examples/kafka-java/#producer-with-callback","title":"Producer with Callback","text":"<pre><code>package com.example.kafka;\n\nimport org.apache.kafka.clients.producer.*;\nimport org.apache.kafka.common.serialization.StringSerializer;\n\nimport java.util.Properties;\nimport java.util.concurrent.Future;\n\npublic class ProducerWithCallback {\n    public static void main(String[] args) {\n        Properties props = new Properties();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        props.put(ProducerConfig.ACKS_CONFIG, \"all\"); // Wait for all replicas\n        props.put(ProducerConfig.RETRIES_CONFIG, 3);\n        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true); // Exactly-once\n\n        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);\n\n        ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(\n            \"my-topic\",\n            \"key-1\",\n            \"Hello Kafka!\"\n        );\n\n        producer.send(record, new Callback() {\n            @Override\n            public void onCompletion(RecordMetadata metadata, Exception exception) {\n                if (exception == null) {\n                    System.out.println(\"Successfully sent to topic: \" + metadata.topic() +\n                                     \", partition: \" + metadata.partition() +\n                                     \", offset: \" + metadata.offset());\n                } else {\n                    System.err.println(\"Error sending message: \" + exception.getMessage());\n                }\n            }\n        });\n\n        producer.flush();\n        producer.close();\n    }\n}\n</code></pre>"},{"location":"examples/kafka-java/#producer-with-custom-serializer","title":"Producer with Custom Serializer","text":"<pre><code>package com.example.kafka;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport org.apache.kafka.common.serialization.Serializer;\n\nimport java.util.Map;\n\npublic class JsonSerializer&lt;T&gt; implements Serializer&lt;T&gt; {\n    private final ObjectMapper objectMapper = new ObjectMapper();\n\n    @Override\n    public void configure(Map&lt;String, ?&gt; configs, boolean isKey) {\n        // Configuration if needed\n    }\n\n    @Override\n    public byte[] serialize(String topic, T data) {\n        if (data == null) {\n            return null;\n        }\n        try {\n            return objectMapper.writeValueAsBytes(data);\n        } catch (Exception e) {\n            throw new RuntimeException(\"Error serializing JSON\", e);\n        }\n    }\n\n    @Override\n    public void close() {\n        // Cleanup if needed\n    }\n}\n</code></pre>"},{"location":"examples/kafka-java/#basic-consumer","title":"Basic Consumer","text":""},{"location":"examples/kafka-java/#simple-consumer","title":"Simple Consumer","text":"<pre><code>package com.example.kafka;\n\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.common.serialization.StringDeserializer;\n\nimport java.time.Duration;\nimport java.util.Collections;\nimport java.util.Properties;\n\npublic class SimpleConsumer {\n    public static void main(String[] args) {\n        Properties props = new Properties();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, \"my-consumer-group\");\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n\n        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);\n        consumer.subscribe(Collections.singletonList(\"my-topic\"));\n\n        try {\n            while (true) {\n                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));\n                for (ConsumerRecord&lt;String, String&gt; record : records) {\n                    System.out.printf(\"Topic: %s, Partition: %d, Offset: %d, Key: %s, Value: %s%n\",\n                        record.topic(),\n                        record.partition(),\n                        record.offset(),\n                        record.key(),\n                        record.value()\n                    );\n                }\n            }\n        } finally {\n            consumer.close();\n        }\n    }\n}\n</code></pre>"},{"location":"examples/kafka-java/#consumer-with-manual-commit","title":"Consumer with Manual Commit","text":"<pre><code>package com.example.kafka;\n\nimport org.apache.kafka.clients.consumer.*;\nimport org.apache.kafka.common.serialization.StringDeserializer;\n\nimport java.time.Duration;\nimport java.util.Collections;\nimport java.util.Properties;\n\npublic class ManualCommitConsumer {\n    public static void main(String[] args) {\n        Properties props = new Properties();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, \"manual-commit-group\");\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // Manual commit\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n\n        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);\n        consumer.subscribe(Collections.singletonList(\"my-topic\"));\n\n        try {\n            while (true) {\n                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));\n\n                for (ConsumerRecord&lt;String, String&gt; record : records) {\n                    try {\n                        // Process message\n                        processMessage(record);\n\n                        // Commit offset after successful processing\n                        consumer.commitSync();\n                    } catch (Exception e) {\n                        System.err.println(\"Error processing message: \" + e.getMessage());\n                        // Don't commit on error - message will be reprocessed\n                    }\n                }\n            }\n        } finally {\n            consumer.close();\n        }\n    }\n\n    private static void processMessage(ConsumerRecord&lt;String, String&gt; record) {\n        System.out.println(\"Processing: \" + record.value());\n        // Your processing logic here\n    }\n}\n</code></pre>"},{"location":"examples/kafka-java/#consumer-group-example","title":"Consumer Group Example","text":""},{"location":"examples/kafka-java/#multiple-consumers-in-group","title":"Multiple Consumers in Group","text":"<pre><code>package com.example.kafka;\n\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.common.serialization.StringDeserializer;\n\nimport java.time.Duration;\nimport java.util.Collections;\nimport java.util.Properties;\n\npublic class ConsumerGroupExample {\n    public static void main(String[] args) {\n        String groupId = \"my-consumer-group\";\n        String topic = \"my-topic\";\n\n        // Consumer 1\n        new Thread(() -&gt; startConsumer(groupId, topic, \"consumer-1\")).start();\n\n        // Consumer 2\n        new Thread(() -&gt; startConsumer(groupId, topic, \"consumer-2\")).start();\n\n        // Consumer 3\n        new Thread(() -&gt; startConsumer(groupId, topic, \"consumer-3\")).start();\n    }\n\n    private static void startConsumer(String groupId, String topic, String consumerId) {\n        Properties props = new Properties();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n\n        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);\n        consumer.subscribe(Collections.singletonList(topic));\n\n        try {\n            while (true) {\n                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(100));\n                for (ConsumerRecord&lt;String, String&gt; record : records) {\n                    System.out.printf(\"[%s] Topic: %s, Partition: %d, Value: %s%n\",\n                        consumerId,\n                        record.topic(),\n                        record.partition(),\n                        record.value()\n                    );\n                }\n            }\n        } finally {\n            consumer.close();\n        }\n    }\n}\n</code></pre>"},{"location":"examples/kafka-java/#advanced-producer","title":"Advanced Producer","text":""},{"location":"examples/kafka-java/#producer-with-partitions","title":"Producer with Partitions","text":"<pre><code>package com.example.kafka;\n\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.Producer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.common.serialization.StringSerializer;\n\nimport java.util.Properties;\n\npublic class PartitionedProducer {\n    public static void main(String[] args) {\n        Properties props = new Properties();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n\n        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);\n\n        // Send to specific partition\n        ProducerRecord&lt;String, String&gt; record1 = new ProducerRecord&lt;&gt;(\n            \"my-topic\",\n            0,  // Partition 0\n            \"key-1\",\n            \"value-1\"\n        );\n\n        // Send with key (Kafka will hash key to determine partition)\n        ProducerRecord&lt;String, String&gt; record2 = new ProducerRecord&lt;&gt;(\n            \"my-topic\",\n            \"user-123\",  // Key - same key goes to same partition\n            \"user-action\"\n        );\n\n        producer.send(record1);\n        producer.send(record2);\n        producer.flush();\n        producer.close();\n    }\n}\n</code></pre>"},{"location":"examples/kafka-java/#producer-with-custom-partitioner","title":"Producer with Custom Partitioner","text":"<pre><code>package com.example.kafka;\n\nimport org.apache.kafka.clients.producer.Partitioner;\nimport org.apache.kafka.common.Cluster;\nimport org.apache.kafka.common.PartitionInfo;\n\nimport java.util.List;\nimport java.util.Map;\n\npublic class CustomPartitioner implements Partitioner {\n    @Override\n    public int partition(String topic, Object key, byte[] keyBytes,\n                        Object value, byte[] valueBytes, Cluster cluster) {\n        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);\n        int numPartitions = partitions.size();\n\n        // Custom partitioning logic\n        if (key == null) {\n            return 0; // Default partition for null keys\n        }\n\n        String keyString = key.toString();\n        // Example: partition by first character\n        return Math.abs(keyString.charAt(0)) % numPartitions;\n    }\n\n    @Override\n    public void close() {\n        // Cleanup\n    }\n\n    @Override\n    public void configure(Map&lt;String, ?&gt; configs) {\n        // Configuration\n    }\n}\n</code></pre>"},{"location":"examples/kafka-java/#error-handling","title":"Error Handling","text":""},{"location":"examples/kafka-java/#producer-with-error-handling","title":"Producer with Error Handling","text":"<pre><code>package com.example.kafka;\n\nimport org.apache.kafka.clients.producer.*;\nimport org.apache.kafka.common.serialization.StringSerializer;\n\nimport java.util.Properties;\nimport java.util.concurrent.ExecutionException;\n\npublic class ProducerWithErrorHandling {\n    public static void main(String[] args) {\n        Properties props = new Properties();\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        props.put(ProducerConfig.RETRIES_CONFIG, 3);\n        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1);\n\n        Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props);\n\n        ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(\n            \"my-topic\",\n            \"key\",\n            \"value\"\n        );\n\n        try {\n            // Synchronous send with error handling\n            RecordMetadata metadata = producer.send(record).get();\n            System.out.println(\"Sent successfully: \" + metadata);\n        } catch (InterruptedException | ExecutionException e) {\n            System.err.println(\"Error sending message: \" + e.getMessage());\n            // Handle error - retry, log, send to DLQ, etc.\n        } finally {\n            producer.close();\n        }\n    }\n}\n</code></pre>"},{"location":"examples/kafka-java/#best-practices","title":"Best Practices","text":""},{"location":"examples/kafka-java/#producer-configuration","title":"Producer Configuration","text":"<pre><code>Properties props = new Properties();\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n\n// Reliability\nprops.put(ProducerConfig.ACKS_CONFIG, \"all\");\nprops.put(ProducerConfig.RETRIES_CONFIG, 3);\nprops.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n\n// Performance\nprops.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, \"snappy\");\nprops.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);\nprops.put(ProducerConfig.LINGER_MS_CONFIG, 10);\n</code></pre>"},{"location":"examples/kafka-java/#consumer-configuration","title":"Consumer Configuration","text":"<pre><code>Properties props = new Properties();\nprops.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\nprops.put(ConsumerConfig.GROUP_ID_CONFIG, \"my-group\");\nprops.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\nprops.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n\n// Reliability\nprops.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);\nprops.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n\n// Performance\nprops.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 100);\nprops.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1);\nprops.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);\n</code></pre>"},{"location":"examples/kafka-java/#running-the-examples","title":"Running the Examples","text":"<p>See the complete Java example project for runnable code.</p>"},{"location":"examples/kafka-java/#further-reading","title":"Further Reading","text":"<ul> <li>Kafka Documentation - Complete Kafka guide</li> <li>Kafka Outbox Pattern - Transactional outbox pattern</li> </ul> <p>Last updated: January 16, 2026</p>"},{"location":"examples/kafka-outbox/","title":"Kafka Outbox Pattern","text":"<p>Implementing the transactional outbox pattern with Kafka.</p>"},{"location":"examples/kafka-outbox/#overview","title":"Overview","text":"<p>The outbox pattern ensures that database changes and message publishing happen atomically, preventing data inconsistencies.</p>"},{"location":"examples/kafka-outbox/#problem","title":"Problem","text":"<p>When you need to: 1. Save data to database 2. Publish event to Kafka</p> <p>If step 2 fails, you have inconsistent state.</p>"},{"location":"examples/kafka-outbox/#solution-outbox-pattern","title":"Solution: Outbox Pattern","text":"<p>Store events in an outbox table within the same database transaction, then publish them asynchronously.</p>"},{"location":"examples/kafka-outbox/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Application \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u251c\u2500\u2500\u25ba Write to main table\n       \u2502\n       \u2514\u2500\u2500\u25ba Write to outbox table\n            \u2502\n            \u2514\u2500\u2500\u25ba Outbox Processor\n                 \u2502\n                 \u2514\u2500\u2500\u25ba Publish to Kafka\n</code></pre>"},{"location":"examples/kafka-outbox/#implementation","title":"Implementation","text":""},{"location":"examples/kafka-outbox/#database-schema","title":"Database Schema","text":"<pre><code>CREATE TABLE orders (\n    id UUID PRIMARY KEY,\n    user_id UUID NOT NULL,\n    amount DECIMAL(10,2) NOT NULL,\n    status VARCHAR(50) NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE outbox (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    aggregate_id UUID NOT NULL,\n    aggregate_type VARCHAR(100) NOT NULL,\n    event_type VARCHAR(100) NOT NULL,\n    payload JSONB NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW(),\n    published BOOLEAN DEFAULT FALSE\n);\n</code></pre>"},{"location":"examples/kafka-outbox/#application-code","title":"Application Code","text":"<pre><code>from sqlalchemy import create_engine, Column, String, JSON, Boolean\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nimport json\nimport uuid\n\nBase = declarative_base()\n\nclass Outbox(Base):\n    __tablename__ = 'outbox'\n\n    id = Column(String, primary_key=True)\n    aggregate_id = Column(String, nullable=False)\n    aggregate_type = Column(String, nullable=False)\n    event_type = Column(String, nullable=False)\n    payload = Column(JSON, nullable=False)\n    published = Column(Boolean, default=False)\n\ndef create_order(user_id, amount):\n    engine = create_engine('postgresql://...')\n    Session = sessionmaker(bind=engine)\n    session = Session()\n\n    try:\n        # Create order\n        order_id = str(uuid.uuid4())\n        order = Order(id=order_id, user_id=user_id, amount=amount)\n        session.add(order)\n\n        # Create outbox event\n        outbox_event = Outbox(\n            id=str(uuid.uuid4()),\n            aggregate_id=order_id,\n            aggregate_type='Order',\n            event_type='OrderCreated',\n            payload={\n                'order_id': order_id,\n                'user_id': user_id,\n                'amount': str(amount)\n            }\n        )\n        session.add(outbox_event)\n\n        # Commit transaction\n        session.commit()\n    except Exception as e:\n        session.rollback()\n        raise\n\n# Outbox Processor\ndef process_outbox():\n    session = Session()\n    unpublished = session.query(Outbox).filter(\n        Outbox.published == False\n    ).limit(100).all()\n\n    for event in unpublished:\n        try:\n            # Publish to Kafka\n            kafka_producer.send('orders', value=event.payload)\n\n            # Mark as published\n            event.published = True\n            session.commit()\n        except Exception as e:\n            session.rollback()\n            # Will retry on next run\n</code></pre>"},{"location":"examples/kafka-outbox/#benefits","title":"Benefits","text":"<ul> <li>\u2705 Atomicity - Database and event in same transaction</li> <li>\u2705 Reliability - Events never lost</li> <li>\u2705 Consistency - No partial updates</li> </ul>"},{"location":"examples/kafka-outbox/#considerations","title":"Considerations","text":"<ul> <li>Outbox processor must be reliable</li> <li>Consider idempotency for consumers</li> <li>Monitor outbox table size</li> </ul>"},{"location":"examples/kafka-outbox/#further-reading","title":"Further Reading","text":"<ul> <li>Kafka</li> <li>Idempotency</li> <li>Examples Code</li> </ul> <p>Last updated: January 16, 2026</p>"},{"location":"examples/rabbitmq-retry-dlx/","title":"RabbitMQ Retry with Dead Letter Exchange","text":"<p>Implementing retry logic with RabbitMQ using Dead Letter Exchanges (DLX).</p>"},{"location":"examples/rabbitmq-retry-dlx/#overview","title":"Overview","text":"<p>This pattern uses RabbitMQ's Dead Letter Exchange feature to implement automatic retries with exponential backoff.</p>"},{"location":"examples/rabbitmq-retry-dlx/#architecture","title":"Architecture","text":"<pre><code>Main Queue \u2192 Process \u2192 Success\n              \u2502\n              \u2514\u2500\u2500\u25ba Failure\n                    \u2502\n                    \u2514\u2500\u2500\u25ba Retry Queue (with TTL)\n                          \u2502\n                          \u2514\u2500\u2500\u25ba After TTL expires\n                                \u2502\n                                \u2514\u2500\u2500\u25ba Back to Main Queue\n                                      \u2502\n                                      \u2514\u2500\u2500\u25ba Max retries exceeded\n                                            \u2502\n                                            \u2514\u2500\u2500\u25ba Dead Letter Queue\n</code></pre>"},{"location":"examples/rabbitmq-retry-dlx/#implementation","title":"Implementation","text":""},{"location":"examples/rabbitmq-retry-dlx/#queue-setup","title":"Queue Setup","text":"<pre><code>import pika\n\nconnection = pika.BlockingConnection(\n    pika.ConnectionParameters('localhost')\n)\nchannel = connection.channel()\n\n# Dead Letter Exchange\nchannel.exchange_declare(\n    exchange='dlx',\n    exchange_type='direct',\n    durable=True\n)\n\n# Dead Letter Queue\nchannel.queue_declare(\n    queue='dlq',\n    durable=True\n)\n\nchannel.queue_bind(\n    exchange='dlx',\n    queue='dlq',\n    routing_key='failed'\n)\n\n# Retry Exchange\nchannel.exchange_declare(\n    exchange='retry-exchange',\n    exchange_type='direct',\n    durable=True\n)\n\n# Retry Queue (with TTL and DLX)\nchannel.queue_declare(\n    queue='retry-queue',\n    durable=True,\n    arguments={\n        'x-dead-letter-exchange': '',\n        'x-dead-letter-routing-key': 'main-queue',\n        'x-message-ttl': 5000  # 5 seconds\n    }\n)\n\nchannel.queue_bind(\n    exchange='retry-exchange',\n    queue='retry-queue',\n    routing_key='retry'\n)\n\n# Main Queue\nchannel.queue_declare(\n    queue='main-queue',\n    durable=True\n)\n</code></pre>"},{"location":"examples/rabbitmq-retry-dlx/#consumer-with-retry-logic","title":"Consumer with Retry Logic","text":"<pre><code>import json\nimport pika\n\nMAX_RETRIES = 3\n\ndef process_message(ch, method, properties, body):\n    try:\n        data = json.loads(body)\n\n        # Simulate processing\n        result = process_data(data)\n\n        # Acknowledge on success\n        ch.basic_ack(delivery_tag=method.delivery_tag)\n\n    except Exception as e:\n        # Get retry count from headers\n        retry_count = properties.headers.get('x-retry-count', 0) if properties.headers else 0\n\n        if retry_count &lt; MAX_RETRIES:\n            # Increment retry count\n            new_headers = properties.headers.copy() if properties.headers else {}\n            new_headers['x-retry-count'] = retry_count + 1\n\n            # Calculate exponential backoff\n            ttl = 1000 * (2 ** retry_count)  # 1s, 2s, 4s, ...\n\n            # Update retry queue TTL\n            channel.queue_declare(\n                queue='retry-queue',\n                durable=True,\n                arguments={\n                    'x-dead-letter-exchange': '',\n                    'x-dead-letter-routing-key': 'main-queue',\n                    'x-message-ttl': ttl\n                }\n            )\n\n            # Publish to retry queue\n            channel.basic_publish(\n                exchange='retry-exchange',\n                routing_key='retry',\n                body=body,\n                properties=pika.BasicProperties(\n                    headers=new_headers,\n                    delivery_mode=2  # Persistent\n                )\n            )\n\n            # Acknowledge original message\n            ch.basic_ack(delivery_tag=method.delivery_tag)\n        else:\n            # Max retries exceeded, send to DLQ\n            channel.basic_publish(\n                exchange='dlx',\n                routing_key='failed',\n                body=body,\n                properties=pika.BasicProperties(\n                    headers=properties.headers,\n                    delivery_mode=2\n                )\n            )\n\n            # Acknowledge original message\n            ch.basic_ack(delivery_tag=method.delivery_tag)\n\n# Start consuming\nchannel.basic_qos(prefetch_count=1)\nchannel.basic_consume(\n    queue='main-queue',\n    on_message_callback=process_message\n)\n\nchannel.start_consuming()\n</code></pre>"},{"location":"examples/rabbitmq-retry-dlx/#alternative-multiple-retry-queues","title":"Alternative: Multiple Retry Queues","text":"<p>For more control, use separate queues for each retry attempt:</p> <pre><code># Retry queues with different TTLs\nretry_queues = [\n    {'name': 'retry-1', 'ttl': 1000},   # 1 second\n    {'name': 'retry-2', 'ttl': 5000},   # 5 seconds\n    {'name': 'retry-3', 'ttl': 30000},  # 30 seconds\n]\n\nfor queue in retry_queues:\n    channel.queue_declare(\n        queue=queue['name'],\n        durable=True,\n        arguments={\n            'x-dead-letter-exchange': '',\n            'x-dead-letter-routing-key': 'main-queue',\n            'x-message-ttl': queue['ttl']\n        }\n    )\n</code></pre>"},{"location":"examples/rabbitmq-retry-dlx/#benefits","title":"Benefits","text":"<ul> <li>\u2705 Automatic retries - No manual intervention</li> <li>\u2705 Exponential backoff - Reduces load on failing services</li> <li>\u2705 Dead letter handling - Failed messages don't get lost</li> <li>\u2705 Configurable - Adjust retry count and delays</li> </ul>"},{"location":"examples/rabbitmq-retry-dlx/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor DLQ size</li> <li>Track retry rates</li> <li>Alert on high failure rates</li> <li>Review DLQ messages regularly</li> </ul>"},{"location":"examples/rabbitmq-retry-dlx/#further-reading","title":"Further Reading","text":"<ul> <li>RabbitMQ</li> <li>Retries and DLQ</li> <li>Examples Code</li> </ul> <p>Last updated: January 16, 2026</p>"},{"location":"examples/rest-api-example/","title":"REST API Example","text":"<p>A complete example of building a REST API with CRUD operations.</p>"},{"location":"examples/rest-api-example/#overview","title":"Overview","text":"<p>This example demonstrates how to build a RESTful API using Flask with proper error handling, validation, and response formatting.</p>"},{"location":"examples/rest-api-example/#code-example","title":"Code Example","text":"<p>The complete example code is available in the <code>examples/api/</code> directory.</p> <pre><code>from flask import Flask, jsonify, request\nfrom flask.views import MethodView\n\napp = Flask(__name__)\n\n# In-memory storage (use a database in production)\nusers = [\n    {\"id\": 1, \"name\": \"John Doe\", \"email\": \"john@example.com\"},\n    {\"id\": 2, \"name\": \"Jane Smith\", \"email\": \"jane@example.com\"},\n]\n\nclass UserAPI(MethodView):\n    \"\"\"User resource endpoints\"\"\"\n\n    def get(self, user_id=None):\n        \"\"\"Get user(s)\"\"\"\n        if user_id:\n            user = next((u for u in users if u[\"id\"] == user_id), None)\n            if not user:\n                return jsonify({\"error\": \"User not found\"}), 404\n            return jsonify(user)\n        return jsonify(users)\n\n    def post(self):\n        \"\"\"Create a new user\"\"\"\n        data = request.get_json()\n        if not data or \"name\" not in data or \"email\" not in data:\n            return jsonify({\"error\": \"Name and email are required\"}), 400\n\n        new_id = max([u[\"id\"] for u in users], default=0) + 1\n        user = {\n            \"id\": new_id,\n            \"name\": data[\"name\"],\n            \"email\": data[\"email\"],\n        }\n        users.append(user)\n        return jsonify(user), 201\n</code></pre>"},{"location":"examples/rest-api-example/#running-the-example","title":"Running the Example","text":"<p>Quick Start</p> <p>Navigate to <code>examples/api/</code> and follow the README instructions.</p> <pre><code>cd examples/api\npip install flask\npython rest-api-example.py\n</code></pre>"},{"location":"examples/rest-api-example/#api-endpoints","title":"API Endpoints","text":"Method Endpoint Description GET <code>/api/users</code> List all users GET <code>/api/users/{id}</code> Get user by ID POST <code>/api/users</code> Create new user PUT <code>/api/users/{id}</code> Update user DELETE <code>/api/users/{id}</code> Delete user"},{"location":"examples/rest-api-example/#testing-with-curl","title":"Testing with cURL","text":"List UsersCreate UserGet User <pre><code>curl http://localhost:5000/api/users\n</code></pre> <pre><code>curl -X POST http://localhost:5000/api/users \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"Alice\", \"email\": \"alice@example.com\"}'\n</code></pre> <pre><code>curl http://localhost:5000/api/users/1\n</code></pre>"},{"location":"examples/rest-api-example/#best-practices","title":"Best Practices","text":"<p>Validation</p> <p>Always validate input data before processing.</p> <p>Security</p> <p>In production, use a proper database and implement authentication.</p> <p>Error Handling</p> <p>Return appropriate HTTP status codes and error messages.</p>"},{"location":"examples/rest-api-example/#next-steps","title":"Next Steps","text":"<ul> <li>Add authentication and authorization</li> <li>Implement database persistence</li> <li>Add request validation with schemas</li> <li>Write unit and integration tests</li> <li>Add API documentation with Swagger/OpenAPI</li> </ul>"},{"location":"foundations/","title":"Foundations","text":"<p>Fundamental concepts and principles for backend engineering.</p>"},{"location":"foundations/#overview","title":"Overview","text":"<p>This section covers the core foundations that every backend engineer should understand, including distributed systems basics, consistency models, and idempotency patterns.</p>"},{"location":"foundations/#topics","title":"Topics","text":"<ul> <li>Distributed Systems Basics - Core concepts of distributed systems</li> <li>Consistency Models - Understanding consistency in distributed systems</li> <li>Idempotency - Designing idempotent operations</li> </ul>"},{"location":"foundations/#why-foundations-matter","title":"Why Foundations Matter","text":"<p>Understanding these foundational concepts is crucial for:</p> <ul> <li>\u2705 Building reliable distributed systems</li> <li>\u2705 Making informed architectural decisions</li> <li>\u2705 Debugging complex issues</li> <li>\u2705 Communicating effectively with your team</li> </ul> <p>Add more foundation topics as needed</p>"},{"location":"foundations/consistency-models/","title":"Consistency Models","text":"<p>Understanding different consistency models in distributed systems.</p>"},{"location":"foundations/consistency-models/#overview","title":"Overview","text":"<p>Consistency models define the guarantees about when updates to data become visible to different nodes in a distributed system.</p>"},{"location":"foundations/consistency-models/#strong-consistency","title":"Strong Consistency","text":"<p>All nodes see the same data at the same time. Updates are atomic and immediately visible.</p> <p>Use Cases</p> <ul> <li>Financial transactions</li> <li>Critical configuration data</li> <li>Leader election</li> </ul>"},{"location":"foundations/consistency-models/#eventual-consistency","title":"Eventual Consistency","text":"<p>Given enough time, all nodes will converge to the same state, but temporary inconsistencies are allowed.</p> <p>Benefits</p> <ul> <li>Better availability</li> <li>Lower latency</li> <li>Higher throughput</li> </ul>"},{"location":"foundations/consistency-models/#consistency-levels","title":"Consistency Levels","text":""},{"location":"foundations/consistency-models/#read-your-writes","title":"Read Your Writes","text":"<p>After writing, you will always read your own writes.</p>"},{"location":"foundations/consistency-models/#monotonic-reads","title":"Monotonic Reads","text":"<p>If you read a value, subsequent reads will return the same or a newer value.</p>"},{"location":"foundations/consistency-models/#causal-consistency","title":"Causal Consistency","text":"<p>Causally related operations are seen in the same order by all nodes.</p>"},{"location":"foundations/consistency-models/#choosing-a-model","title":"Choosing a Model","text":"<p>Consider:</p> <ul> <li>Latency requirements: Strong consistency may increase latency</li> <li>Availability needs: Eventual consistency improves availability</li> <li>Data criticality: Critical data may need strong consistency</li> </ul>"},{"location":"foundations/consistency-models/#examples","title":"Examples","text":"Strong ConsistencyEventual Consistency <pre><code># All replicas must agree before returning\ndef write(key, value):\n    consensus = get_consensus_from_all_replicas()\n    if consensus:\n        return write_to_all_replicas(key, value)\n</code></pre> <pre><code># Write to primary, replicate asynchronously\ndef write(key, value):\n    write_to_primary(key, value)\n    replicate_async(key, value)  # Eventually reaches all replicas\n</code></pre>"},{"location":"foundations/consistency-models/#further-reading","title":"Further Reading","text":"<ul> <li>Distributed Systems Basics</li> <li>Messaging Patterns</li> </ul> <p>Last updated: January 16, 2026</p>"},{"location":"foundations/distributed-systems-basics/","title":"Distributed Systems Basics","text":"<p>Fundamental concepts for understanding and building distributed systems.</p>"},{"location":"foundations/distributed-systems-basics/#what-is-a-distributed-system","title":"What is a Distributed System?","text":"<p>A distributed system is a collection of independent computers that appear to users as a single coherent system.</p>"},{"location":"foundations/distributed-systems-basics/#key-characteristics","title":"Key Characteristics","text":""},{"location":"foundations/distributed-systems-basics/#scalability","title":"Scalability","text":"<p>Distributed systems can scale horizontally by adding more nodes.</p> <p>Horizontal vs Vertical Scaling</p> <ul> <li>Horizontal: Add more machines</li> <li>Vertical: Add more resources to existing machines</li> </ul>"},{"location":"foundations/distributed-systems-basics/#fault-tolerance","title":"Fault Tolerance","text":"<p>Systems should continue operating even when some components fail.</p>"},{"location":"foundations/distributed-systems-basics/#consistency","title":"Consistency","text":"<p>Different consistency models trade off between performance and correctness.</p>"},{"location":"foundations/distributed-systems-basics/#common-challenges","title":"Common Challenges","text":""},{"location":"foundations/distributed-systems-basics/#network-partitions","title":"Network Partitions","text":"<p>When network links fail, nodes may be unable to communicate.</p>"},{"location":"foundations/distributed-systems-basics/#clock-synchronization","title":"Clock Synchronization","text":"<p>Distributed systems often need to coordinate time across nodes.</p>"},{"location":"foundations/distributed-systems-basics/#consensus","title":"Consensus","text":"<p>Agreeing on a value or decision across multiple nodes.</p>"},{"location":"foundations/distributed-systems-basics/#cap-theorem","title":"CAP Theorem","text":"<p>The CAP theorem states that a distributed system can guarantee at most two of:</p> <ul> <li>Consistency: All nodes see the same data simultaneously</li> <li>Availability: System remains operational</li> <li>Partition Tolerance: System continues despite network failures</li> </ul>"},{"location":"foundations/distributed-systems-basics/#further-reading","title":"Further Reading","text":"<ul> <li>Consistency Models</li> <li>Idempotency</li> </ul> <p>Last updated: January 16, 2026</p>"},{"location":"foundations/idempotency/","title":"Idempotency","text":"<p>Designing operations that can be safely retried.</p>"},{"location":"foundations/idempotency/#what-is-idempotency","title":"What is Idempotency?","text":"<p>An idempotent operation produces the same result regardless of how many times it's executed.</p>"},{"location":"foundations/idempotency/#why-idempotency-matters","title":"Why Idempotency Matters","text":"<p>In distributed systems, operations may be retried due to:</p> <ul> <li>Network failures</li> <li>Timeouts</li> <li>Client retries</li> <li>Message queue redelivery</li> </ul>"},{"location":"foundations/idempotency/#idempotent-operations","title":"Idempotent Operations","text":""},{"location":"foundations/idempotency/#http-methods","title":"HTTP Methods","text":"<ul> <li><code>GET</code> - Always idempotent</li> <li><code>PUT</code> - Idempotent (replacing resource)</li> <li><code>DELETE</code> - Idempotent</li> <li><code>POST</code> - Not idempotent by default</li> </ul>"},{"location":"foundations/idempotency/#database-operations","title":"Database Operations","text":"<pre><code>-- Idempotent: Multiple executions have same effect\nUPDATE users SET status = 'active' WHERE id = 1;\n\n-- Not idempotent: Each execution increments\nUPDATE users SET count = count + 1 WHERE id = 1;\n</code></pre>"},{"location":"foundations/idempotency/#implementing-idempotency","title":"Implementing Idempotency","text":""},{"location":"foundations/idempotency/#idempotency-keys","title":"Idempotency Keys","text":"<p>Use unique keys to track operations:</p> <pre><code>def process_payment(idempotency_key, amount):\n    # Check if already processed\n    if is_processed(idempotency_key):\n        return get_previous_result(idempotency_key)\n\n    # Process payment\n    result = charge_card(amount)\n\n    # Store result\n    store_result(idempotency_key, result)\n    return result\n</code></pre>"},{"location":"foundations/idempotency/#idempotency-in-apis","title":"Idempotency in APIs","text":"<pre><code>@app.post(\"/api/orders\")\ndef create_order(request: OrderRequest):\n    idempotency_key = request.headers.get(\"Idempotency-Key\")\n\n    if idempotency_key:\n        existing = get_order_by_key(idempotency_key)\n        if existing:\n            return existing\n\n    order = create_new_order(request, idempotency_key)\n    return order\n</code></pre>"},{"location":"foundations/idempotency/#best-practices","title":"Best Practices","text":"<ol> <li>Always use idempotency keys for critical operations</li> <li>Store results to return same response on retry</li> <li>Set expiration for idempotency keys</li> <li>Document idempotent endpoints clearly</li> </ol>"},{"location":"foundations/idempotency/#common-patterns","title":"Common Patterns","text":"<ul> <li>Idempotency tokens in request headers</li> <li>Deduplication tables in databases</li> <li>Idempotency middleware in API frameworks</li> </ul>"},{"location":"foundations/idempotency/#further-reading","title":"Further Reading","text":"<ul> <li>Distributed Systems Basics</li> <li>Retries and DLQ</li> </ul> <p>Last updated: January 16, 2026</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you set up the documentation environment locally.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>pip (Python package manager)</li> <li>Git</li> </ul>"},{"location":"getting-started/installation/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/nvnhan221/backend-engineering-notes.git\ncd backend-engineering-notes\n</code></pre>"},{"location":"getting-started/installation/#step-2-create-virtual-environment","title":"Step 2: Create Virtual Environment","text":"<p>It's recommended to use a virtual environment to isolate dependencies:</p> <pre><code>python3 -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre>"},{"location":"getting-started/installation/#step-3-install-dependencies","title":"Step 3: Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#step-4-verify-installation","title":"Step 4: Verify Installation","text":"<p>Check that MkDocs is installed correctly:</p> <pre><code>mkdocs --version\n</code></pre>"},{"location":"getting-started/installation/#step-5-serve-locally","title":"Step 5: Serve Locally","text":"<p>Start the development server:</p> <pre><code>mkdocs serve\n</code></pre> <p>The documentation will be available at <code>http://127.0.0.1:8000</code></p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Read the Quick Start Guide to learn how to write documentation</li> <li>Check out the Examples for code samples</li> <li>Explore the Guides for detailed tutorials</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Learn how to write and build documentation with MkDocs Material.</p>"},{"location":"getting-started/quick-start/#writing-documentation","title":"Writing Documentation","text":"<p>All documentation is written in Markdown and stored in the <code>docs/</code> directory.</p>"},{"location":"getting-started/quick-start/#basic-markdown","title":"Basic Markdown","text":"<pre><code># Heading 1\n## Heading 2\n### Heading 3\n\n**Bold text** and *italic text*\n\n- List item 1\n- List item 2\n\n1. Numbered item 1\n2. Numbered item 2\n</code></pre>"},{"location":"getting-started/quick-start/#code-blocks","title":"Code Blocks","text":"<pre><code>```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n</code></pre>"},{"location":"getting-started/quick-start/#admonitions","title":"Admonitions","text":"<p>Note</p> <p>This is a note admonition.</p> <p>Warning</p> <p>This is a warning.</p> <p>Tip</p> <p>This is a helpful tip.</p>"},{"location":"getting-started/quick-start/#tabs","title":"Tabs","text":"PythonJavaScript <pre><code>print(\"Hello from Python\")\n</code></pre> <pre><code>console.log(\"Hello from JavaScript\");\n</code></pre>"},{"location":"getting-started/quick-start/#building-the-site","title":"Building the Site","text":""},{"location":"getting-started/quick-start/#development-server","title":"Development Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This starts a local development server with live reload.</p>"},{"location":"getting-started/quick-start/#build-static-site","title":"Build Static Site","text":"<pre><code>mkdocs build\n</code></pre> <p>This generates a static site in the <code>site/</code> directory.</p>"},{"location":"getting-started/quick-start/#deploy","title":"Deploy","text":"<pre><code>mkdocs gh-deploy\n</code></pre> <p>This deploys the site to GitHub Pages.</p>"},{"location":"getting-started/quick-start/#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 docs/              # Documentation source files\n\u2502   \u251c\u2500\u2500 index.md       # Home page\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 examples/          # Code examples\n\u251c\u2500\u2500 mkdocs.yml         # MkDocs configuration\n\u2514\u2500\u2500 requirements.txt   # Python dependencies\n</code></pre>"},{"location":"getting-started/quick-start/#adding-new-pages","title":"Adding New Pages","text":"<ol> <li>Create a new <code>.md</code> file in the <code>docs/</code> directory</li> <li>Add it to the <code>nav</code> section in <code>mkdocs.yml</code></li> <li>Write your content in Markdown</li> <li>The page will appear in the navigation automatically</li> </ol>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the Examples for more advanced features</li> <li>Read the Guides for detailed tutorials</li> <li>Check the MkDocs Material documentation for more features</li> </ul>"},{"location":"guides/","title":"Guides","text":"<p>Comprehensive guides and tutorials for backend engineering.</p>"},{"location":"guides/#available-guides","title":"Available Guides","text":""},{"location":"guides/#architecture","title":"Architecture","text":"<ul> <li>Microservices Architecture - Learn how to design scalable microservices</li> <li>API Design - Best practices for RESTful API design</li> <li>Database Design - Database modeling and optimization</li> </ul>"},{"location":"guides/#development","title":"Development","text":"<ul> <li>Code Organization - Structuring your codebase</li> <li>Testing Strategies - Unit, integration, and E2E testing</li> <li>Error Handling - Robust error handling patterns</li> </ul>"},{"location":"guides/#devops","title":"DevOps","text":"<ul> <li>CI/CD Pipelines - Setting up continuous integration and deployment</li> <li>Containerization - Docker and Kubernetes basics</li> <li>Monitoring - Application monitoring and logging</li> </ul>"},{"location":"guides/#contributing-guides","title":"Contributing Guides","text":"<p>Want to add a new guide? Follow these steps:</p> <ol> <li>Create a new Markdown file in <code>docs/guides/</code></li> <li>Add it to the navigation in <code>mkdocs.yml</code></li> <li>Follow the existing guide structure</li> <li>Submit a pull request</li> </ol> <p>More guides coming soon!</p>"},{"location":"messaging/","title":"Messaging","text":"<p>Message queues and event-driven architectures.</p>"},{"location":"messaging/#overview","title":"Overview","text":"<p>This section covers messaging systems, patterns, and best practices for building event-driven applications.</p>"},{"location":"messaging/#topics","title":"Topics","text":"<ul> <li>Kafka - Apache Kafka fundamentals and patterns</li> <li>RabbitMQ - RabbitMQ usage and patterns</li> <li>Retries and DLQ - Handling failures and dead letter queues</li> </ul>"},{"location":"messaging/#when-to-use-messaging","title":"When to Use Messaging","text":"<p>Messaging is ideal for:</p> <ul> <li>Asynchronous processing - Decouple producers and consumers</li> <li>Event-driven architecture - React to events across services</li> <li>Load leveling - Smooth out traffic spikes</li> <li>Reliability - Guaranteed message delivery</li> </ul>"},{"location":"messaging/#common-patterns","title":"Common Patterns","text":""},{"location":"messaging/#publish-subscribe","title":"Publish-Subscribe","text":"<p>Multiple consumers receive the same message.</p>"},{"location":"messaging/#point-to-point","title":"Point-to-Point","text":"<p>One message is consumed by exactly one consumer.</p>"},{"location":"messaging/#request-reply","title":"Request-Reply","text":"<p>Synchronous request-response pattern over messaging.</p>"},{"location":"messaging/#further-reading","title":"Further Reading","text":"<ul> <li>Examples - Practical examples</li> <li>Foundations - Core concepts</li> </ul> <p>Add more messaging topics as needed</p>"},{"location":"messaging/kafka/","title":"Kafka","text":"<p>Apache Kafka for event streaming and messaging.</p>"},{"location":"messaging/kafka/#overview","title":"Overview","text":"<p>Apache Kafka is a distributed event streaming platform capable of handling trillions of events per day. It's designed to handle high-throughput, low-latency real-time data feeds.</p>"},{"location":"messaging/kafka/#key-concepts","title":"Key Concepts","text":""},{"location":"messaging/kafka/#topics","title":"Topics","text":"<p>Topics are categories or feeds to which records are published. A topic is identified by its name.</p>"},{"location":"messaging/kafka/#partitions","title":"Partitions","text":"<p>Topics are split into partitions for parallelism and scalability. Each partition is an ordered, immutable sequence of records.</p>"},{"location":"messaging/kafka/#producers","title":"Producers","text":"<p>Applications that publish data to topics. Producers choose which partition to send data to.</p>"},{"location":"messaging/kafka/#consumers","title":"Consumers","text":"<p>Applications that read data from topics. Consumers read from one or more partitions.</p>"},{"location":"messaging/kafka/#consumer-groups","title":"Consumer Groups","text":"<p>Multiple consumers working together to consume a topic. Each consumer in a group reads from a different partition.</p>"},{"location":"messaging/kafka/#brokers","title":"Brokers","text":"<p>Kafka cluster consists of one or more servers (brokers). Each broker can handle terabytes of data.</p>"},{"location":"messaging/kafka/#replication","title":"Replication","text":"<p>Partitions are replicated across multiple brokers for fault tolerance.</p>"},{"location":"messaging/kafka/#installation","title":"Installation","text":""},{"location":"messaging/kafka/#using-docker","title":"Using Docker","text":"<pre><code>docker run -d \\\n  --name kafka \\\n  -p 9092:9092 \\\n  -e KAFKA_ZOOKEEPER_CONNECT=localhost:2181 \\\n  apache/kafka:latest\n</code></pre>"},{"location":"messaging/kafka/#using-docker-compose","title":"Using Docker Compose","text":"<pre><code>version: '3.8'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:latest\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n  kafka:\n    image: confluentinc/cp-kafka:latest\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n</code></pre>"},{"location":"messaging/kafka/#local-installation","title":"Local Installation","text":"<ol> <li>Download Kafka from kafka.apache.org</li> <li>Extract and navigate to the directory</li> <li>Start Zookeeper:    <pre><code>bin/zookeeper-server-start.sh config/zookeeper.properties\n</code></pre></li> <li>Start Kafka:    <pre><code>bin/kafka-server-start.sh config/server.properties\n</code></pre></li> </ol>"},{"location":"messaging/kafka/#architecture","title":"Architecture","text":""},{"location":"messaging/kafka/#cluster-architecture","title":"Cluster Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Producer   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Kafka Broker   \u2502\u2500\u2500\u2510\n\u2502  (Partition 0)  \u2502  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n                     \u2502 Replication\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  Kafka Broker   \u2502\u2500\u2500\u2518\n\u2502  (Partition 1)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Consumer   \u2502\n\u2502   Group     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"messaging/kafka/#partitioning","title":"Partitioning","text":"<ul> <li>Messages with the same key go to the same partition</li> <li>Ordering is guaranteed within a partition</li> <li>Parallelism is achieved through multiple partitions</li> </ul>"},{"location":"messaging/kafka/#replication-factor","title":"Replication Factor","text":"<ul> <li>Replication Factor = 1: No redundancy (not recommended for production)</li> <li>Replication Factor = 3: Standard for production (can tolerate 2 broker failures)</li> </ul>"},{"location":"messaging/kafka/#in-sync-replicas-isr","title":"In-Sync Replicas (ISR)","text":"<p>ISR are replicas that are caught up with the leader. Only ISR can become leaders.</p>"},{"location":"messaging/kafka/#basic-usage","title":"Basic Usage","text":""},{"location":"messaging/kafka/#producer-example-python","title":"Producer Example (Python)","text":"<pre><code>from kafka import KafkaProducer\nimport json\n\nproducer = KafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\nproducer.send('my-topic', {'key': 'value'})\nproducer.flush()\n</code></pre>"},{"location":"messaging/kafka/#consumer-example-python","title":"Consumer Example (Python)","text":"<pre><code>from kafka import KafkaConsumer\nimport json\n\nconsumer = KafkaConsumer(\n    'my-topic',\n    bootstrap_servers=['localhost:9092'],\n    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n    group_id='my-group'\n)\n\nfor message in consumer:\n    print(f\"Received: {message.value}\")\n</code></pre>"},{"location":"messaging/kafka/#java-examples","title":"Java Examples","text":"<p>See Kafka Java Examples for complete Java implementations.</p>"},{"location":"messaging/kafka/#advanced-concepts","title":"Advanced Concepts","text":""},{"location":"messaging/kafka/#message-keys","title":"Message Keys","text":"<p>Using keys ensures messages with the same key go to the same partition:</p> <pre><code>producer.send('my-topic', key=b'user-123', value={'action': 'login'})\n</code></pre>"},{"location":"messaging/kafka/#consumer-offsets","title":"Consumer Offsets","text":"<ul> <li>Kafka stores the offset of the last consumed message</li> <li>Consumers can start from <code>earliest</code> or <code>latest</code></li> <li>Manual offset management for exactly-once processing</li> </ul>"},{"location":"messaging/kafka/#consumer-groups_1","title":"Consumer Groups","text":"<pre><code># Multiple consumers in the same group share partitions\nconsumer1 = KafkaConsumer('my-topic', group_id='my-group')\nconsumer2 = KafkaConsumer('my-topic', group_id='my-group')\n# Each consumer reads from different partitions\n</code></pre>"},{"location":"messaging/kafka/#exactly-once-semantics","title":"Exactly-Once Semantics","text":"<ul> <li>Enable idempotent producer</li> <li>Use transactional producer</li> <li>Read committed messages only</li> </ul>"},{"location":"messaging/kafka/#configuration","title":"Configuration","text":""},{"location":"messaging/kafka/#producer-configuration","title":"Producer Configuration","text":"<pre><code>producer = KafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    acks='all',  # Wait for all replicas to acknowledge\n    retries=3,\n    max_in_flight_requests_per_connection=1,\n    enable_idempotence=True,  # Exactly-once semantics\n    compression_type='snappy',\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n</code></pre>"},{"location":"messaging/kafka/#consumer-configuration","title":"Consumer Configuration","text":"<pre><code>consumer = KafkaConsumer(\n    'my-topic',\n    bootstrap_servers=['localhost:9092'],\n    group_id='my-group',\n    auto_offset_reset='earliest',  # or 'latest'\n    enable_auto_commit=False,  # Manual commit for reliability\n    max_poll_records=100,\n    session_timeout_ms=30000,\n    heartbeat_interval_ms=10000\n)\n</code></pre>"},{"location":"messaging/kafka/#important-settings","title":"Important Settings","text":"Setting Description Recommended Value <code>acks</code> Producer acknowledgment <code>all</code> for reliability <code>retries</code> Number of retries <code>3</code> or higher <code>enable_idempotence</code> Exactly-once semantics <code>True</code> <code>auto_offset_reset</code> Where to start reading <code>earliest</code> or <code>latest</code> <code>enable_auto_commit</code> Auto commit offsets <code>False</code> for reliability"},{"location":"messaging/kafka/#best-practices","title":"Best Practices","text":""},{"location":"messaging/kafka/#partitioning-strategy","title":"Partitioning Strategy","text":"<ul> <li>Use meaningful keys for partitioning</li> <li>Balance partition count with consumer parallelism</li> <li>Consider partition size limits (recommended: &lt; 2GB per partition)</li> <li>More partitions = more parallelism but more overhead</li> </ul>"},{"location":"messaging/kafka/#producer-best-practices","title":"Producer Best Practices","text":"<ul> <li>Use idempotent producer for exactly-once</li> <li>Set appropriate <code>acks</code> based on requirements</li> <li>Use compression (snappy, gzip, lz4)</li> <li>Batch messages for better throughput</li> <li>Handle errors and retries properly</li> </ul>"},{"location":"messaging/kafka/#consumer-best-practices","title":"Consumer Best Practices","text":"<ul> <li>Use consumer groups for parallel processing</li> <li>Commit offsets manually for reliability</li> <li>Handle rebalancing gracefully</li> <li>Monitor consumer lag</li> <li>Set appropriate <code>max_poll_records</code></li> </ul>"},{"location":"messaging/kafka/#error-handling","title":"Error Handling","text":"<ul> <li>Implement retry logic with exponential backoff</li> <li>Use dead letter topics for failed messages</li> <li>Monitor consumer lag</li> <li>Handle deserialization errors</li> <li>Log errors for debugging</li> </ul>"},{"location":"messaging/kafka/#monitoring","title":"Monitoring","text":""},{"location":"messaging/kafka/#key-metrics","title":"Key Metrics","text":"<ul> <li>Throughput: Messages per second</li> <li>Latency: End-to-end message latency</li> <li>Consumer Lag: Delay between producer and consumer</li> <li>Broker Metrics: CPU, memory, disk I/O</li> <li>Partition Count: Number of partitions per topic</li> </ul>"},{"location":"messaging/kafka/#tools","title":"Tools","text":"<ul> <li>Kafka Manager - Web-based management</li> <li>Confluent Control Center - Enterprise monitoring</li> <li>Kafka Exporter - Prometheus metrics</li> <li>Burrow - Consumer lag monitoring</li> </ul>"},{"location":"messaging/kafka/#performance-tuning","title":"Performance Tuning","text":""},{"location":"messaging/kafka/#producer-tuning","title":"Producer Tuning","text":"<pre><code>producer = KafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    batch_size=16384,  # Increase batch size\n    linger_ms=10,  # Wait for batch\n    compression_type='snappy',\n    buffer_memory=33554432  # 32MB buffer\n)\n</code></pre>"},{"location":"messaging/kafka/#consumer-tuning","title":"Consumer Tuning","text":"<pre><code>consumer = KafkaConsumer(\n    'my-topic',\n    bootstrap_servers=['localhost:9092'],\n    fetch_min_bytes=1,\n    fetch_max_wait_ms=500,\n    max_partition_fetch_bytes=1048576  # 1MB per partition\n)\n</code></pre>"},{"location":"messaging/kafka/#use-cases","title":"Use Cases","text":"<ul> <li>Event Sourcing - Store all events as a log</li> <li>Stream Processing - Real-time data processing</li> <li>Log Aggregation - Centralized logging</li> <li>Metrics Collection - Collecting metrics from multiple sources</li> <li>Commit Log - Database replication</li> <li>Message Queue - Decoupling producers and consumers</li> </ul>"},{"location":"messaging/kafka/#common-patterns","title":"Common Patterns","text":""},{"location":"messaging/kafka/#outbox-pattern","title":"Outbox Pattern","text":"<p>Ensure database writes and Kafka publishes happen atomically. See Kafka Outbox Pattern.</p>"},{"location":"messaging/kafka/#cqrs","title":"CQRS","text":"<p>Command Query Responsibility Segregation using Kafka as event store.</p>"},{"location":"messaging/kafka/#event-sourcing","title":"Event Sourcing","text":"<p>Store all state changes as events in Kafka.</p>"},{"location":"messaging/kafka/#troubleshooting","title":"Troubleshooting","text":""},{"location":"messaging/kafka/#consumer-lag","title":"Consumer Lag","text":"<ul> <li>Increase consumer instances</li> <li>Optimize processing logic</li> <li>Check network latency</li> <li>Review partition count</li> </ul>"},{"location":"messaging/kafka/#message-loss","title":"Message Loss","text":"<ul> <li>Set <code>acks='all'</code> in producer</li> <li>Use replication factor &gt;= 3</li> <li>Monitor ISR status</li> <li>Check broker disk space</li> </ul>"},{"location":"messaging/kafka/#performance-issues","title":"Performance Issues","text":"<ul> <li>Increase partition count</li> <li>Tune batch sizes</li> <li>Use compression</li> <li>Optimize serialization</li> </ul>"},{"location":"messaging/kafka/#further-reading","title":"Further Reading","text":"<ul> <li>Kafka Java Examples - Complete Java implementations</li> <li>Kafka Outbox Pattern - Transactional outbox pattern</li> <li>Retries and DLQ - Error handling patterns</li> </ul>"},{"location":"messaging/kafka/#resources","title":"Resources","text":"<ul> <li>Official Documentation</li> <li>Confluent Documentation</li> <li>Kafka Best Practices</li> </ul> <p>Last updated: January 16, 2026</p>"},{"location":"messaging/rabbitmq/","title":"RabbitMQ","text":"<p>RabbitMQ message broker patterns and best practices.</p>"},{"location":"messaging/rabbitmq/#overview","title":"Overview","text":"<p>RabbitMQ is a message broker that implements the Advanced Message Queuing Protocol (AMQP).</p>"},{"location":"messaging/rabbitmq/#key-concepts","title":"Key Concepts","text":""},{"location":"messaging/rabbitmq/#exchanges","title":"Exchanges","text":"<p>Exchanges receive messages and route them to queues.</p>"},{"location":"messaging/rabbitmq/#queues","title":"Queues","text":"<p>Queues store messages until consumed.</p>"},{"location":"messaging/rabbitmq/#bindings","title":"Bindings","text":"<p>Bindings connect exchanges to queues with routing rules.</p>"},{"location":"messaging/rabbitmq/#routing-keys","title":"Routing Keys","text":"<p>Keys used to route messages from exchanges to queues.</p>"},{"location":"messaging/rabbitmq/#exchange-types","title":"Exchange Types","text":""},{"location":"messaging/rabbitmq/#direct-exchange","title":"Direct Exchange","text":"<p>Routes messages to queues based on exact routing key match.</p>"},{"location":"messaging/rabbitmq/#topic-exchange","title":"Topic Exchange","text":"<p>Routes messages using pattern matching on routing keys.</p>"},{"location":"messaging/rabbitmq/#fanout-exchange","title":"Fanout Exchange","text":"<p>Broadcasts messages to all bound queues.</p>"},{"location":"messaging/rabbitmq/#headers-exchange","title":"Headers Exchange","text":"<p>Routes based on message headers instead of routing keys.</p>"},{"location":"messaging/rabbitmq/#basic-usage","title":"Basic Usage","text":""},{"location":"messaging/rabbitmq/#producer-example","title":"Producer Example","text":"<pre><code>import pika\n\nconnection = pika.BlockingConnection(\n    pika.ConnectionParameters('localhost')\n)\nchannel = connection.channel()\n\nchannel.queue_declare(queue='hello')\n\nchannel.basic_publish(\n    exchange='',\n    routing_key='hello',\n    body='Hello World!'\n)\n\nconnection.close()\n</code></pre>"},{"location":"messaging/rabbitmq/#consumer-example","title":"Consumer Example","text":"<pre><code>import pika\n\nconnection = pika.BlockingConnection(\n    pika.ConnectionParameters('localhost')\n)\nchannel = connection.channel()\n\nchannel.queue_declare(queue='hello')\n\ndef callback(ch, method, properties, body):\n    print(f\"Received: {body.decode()}\")\n\nchannel.basic_consume(\n    queue='hello',\n    on_message_callback=callback,\n    auto_ack=True\n)\n\nchannel.start_consuming()\n</code></pre>"},{"location":"messaging/rabbitmq/#best-practices","title":"Best Practices","text":""},{"location":"messaging/rabbitmq/#message-acknowledgment","title":"Message Acknowledgment","text":"<pre><code>def callback(ch, method, properties, body):\n    try:\n        process_message(body)\n        ch.basic_ack(delivery_tag=method.delivery_tag)\n    except Exception as e:\n        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)\n</code></pre>"},{"location":"messaging/rabbitmq/#durable-queues","title":"Durable Queues","text":"<pre><code>channel.queue_declare(\n    queue='my-queue',\n    durable=True  # Survives broker restart\n)\n</code></pre>"},{"location":"messaging/rabbitmq/#prefetch-count","title":"Prefetch Count","text":"<pre><code>channel.basic_qos(prefetch_count=1)  # Fair dispatch\n</code></pre>"},{"location":"messaging/rabbitmq/#use-cases","title":"Use Cases","text":"<ul> <li>Task Queues - Background job processing</li> <li>Work Queues - Distributing work across workers</li> <li>Pub/Sub - Broadcasting messages</li> <li>RPC - Request-reply pattern</li> </ul>"},{"location":"messaging/rabbitmq/#further-reading","title":"Further Reading","text":"<ul> <li>Retries and DLQ</li> <li>RabbitMQ Retry DLX Pattern</li> </ul> <p>Last updated: January 16, 2026</p>"},{"location":"messaging/retries-dlq/","title":"Retries and Dead Letter Queues","text":"<p>Handling message processing failures with retries and dead letter queues.</p>"},{"location":"messaging/retries-dlq/#overview","title":"Overview","text":"<p>When message processing fails, we need strategies to handle retries and eventually move messages to a dead letter queue (DLQ) for manual inspection.</p>"},{"location":"messaging/retries-dlq/#retry-strategies","title":"Retry Strategies","text":""},{"location":"messaging/retries-dlq/#exponential-backoff","title":"Exponential Backoff","text":"<p>Gradually increase delay between retries:</p> <pre><code>import time\nfrom functools import wraps\n\ndef exponential_backoff(max_retries=5):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            for attempt in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except Exception as e:\n                    if attempt == max_retries - 1:\n                        raise\n                    delay = 2 ** attempt\n                    time.sleep(delay)\n            return None\n        return wrapper\n    return decorator\n</code></pre>"},{"location":"messaging/retries-dlq/#fixed-delay","title":"Fixed Delay","text":"<p>Retry with constant delay between attempts.</p>"},{"location":"messaging/retries-dlq/#immediate-retry","title":"Immediate Retry","text":"<p>Retry immediately (use with caution).</p>"},{"location":"messaging/retries-dlq/#dead-letter-queues","title":"Dead Letter Queues","text":"<p>Messages that fail after all retries are moved to a DLQ.</p>"},{"location":"messaging/retries-dlq/#rabbitmq-dlx","title":"RabbitMQ DLX","text":"<pre><code># Declare DLX\nchannel.exchange_declare(\n    exchange='dlx',\n    exchange_type='direct'\n)\n\n# Declare DLQ\nchannel.queue_declare(\n    queue='dlq',\n    durable=True\n)\n\n# Bind DLQ to DLX\nchannel.queue_bind(\n    exchange='dlx',\n    queue='dlq',\n    routing_key='failed'\n)\n\n# Declare main queue with DLX\nchannel.queue_declare(\n    queue='main-queue',\n    durable=True,\n    arguments={\n        'x-dead-letter-exchange': 'dlx',\n        'x-dead-letter-routing-key': 'failed',\n        'x-message-ttl': 60000  # 60 seconds\n    }\n)\n</code></pre>"},{"location":"messaging/retries-dlq/#kafka-dead-letter-topic","title":"Kafka Dead Letter Topic","text":"<pre><code>from kafka import KafkaProducer, KafkaConsumer\n\n# Producer for DLQ\ndlq_producer = KafkaProducer(\n    bootstrap_servers=['localhost:9092']\n)\n\n# Consumer with DLQ handling\nconsumer = KafkaConsumer(\n    'main-topic',\n    bootstrap_servers=['localhost:9092'],\n    group_id='my-group'\n)\n\nfor message in consumer:\n    try:\n        process_message(message.value)\n        consumer.commit()\n    except Exception as e:\n        retry_count = get_retry_count(message)\n        if retry_count &gt;= MAX_RETRIES:\n            # Send to DLQ\n            dlq_producer.send('dlq-topic', message.value)\n        else:\n            # Retry logic\n            retry_message(message, retry_count + 1)\n</code></pre>"},{"location":"messaging/retries-dlq/#best-practices","title":"Best Practices","text":""},{"location":"messaging/retries-dlq/#retry-limits","title":"Retry Limits","text":"<p>Set maximum retry attempts to prevent infinite loops.</p>"},{"location":"messaging/retries-dlq/#monitoring","title":"Monitoring","text":"<p>Monitor DLQ size and investigate failed messages regularly.</p>"},{"location":"messaging/retries-dlq/#alerting","title":"Alerting","text":"<p>Set up alerts when DLQ grows beyond threshold.</p>"},{"location":"messaging/retries-dlq/#manual-review","title":"Manual Review","text":"<p>Regularly review DLQ messages to identify patterns.</p>"},{"location":"messaging/retries-dlq/#patterns","title":"Patterns","text":""},{"location":"messaging/retries-dlq/#outbox-pattern","title":"Outbox Pattern","text":"<p>Use transactional outbox to ensure message delivery.</p>"},{"location":"messaging/retries-dlq/#saga-pattern","title":"Saga Pattern","text":"<p>Handle distributed transactions with compensating actions.</p>"},{"location":"messaging/retries-dlq/#further-reading","title":"Further Reading","text":"<ul> <li>Kafka Outbox Pattern</li> <li>RabbitMQ Retry DLX Pattern</li> <li>Idempotency</li> </ul> <p>Last updated: January 16, 2026</p>"},{"location":"microservices/","title":"Microservices","text":"<p>Microservices architecture patterns and best practices.</p>"},{"location":"microservices/#overview","title":"Overview","text":"<p>This section covers microservices architecture, service design, communication patterns, and operational concerns.</p>"},{"location":"microservices/#topics","title":"Topics","text":"<ul> <li>Service decomposition</li> <li>Service communication</li> <li>API gateway patterns</li> <li>Service mesh</li> <li>Distributed tracing</li> <li>Service discovery</li> </ul>"},{"location":"microservices/#key-concepts","title":"Key Concepts","text":""},{"location":"microservices/#service-boundaries","title":"Service Boundaries","text":"<p>How to identify and define service boundaries.</p>"},{"location":"microservices/#communication-patterns","title":"Communication Patterns","text":"<ul> <li>Synchronous (REST, gRPC)</li> <li>Asynchronous (Message queues, Events)</li> </ul>"},{"location":"microservices/#data-management","title":"Data Management","text":"<ul> <li>Database per service</li> <li>Saga pattern for distributed transactions</li> <li>Event-driven data synchronization</li> </ul>"},{"location":"microservices/#challenges","title":"Challenges","text":"<ul> <li>Service coordination</li> <li>Distributed transactions</li> <li>Network latency</li> <li>Service versioning</li> <li>Testing distributed systems</li> </ul>"},{"location":"microservices/#patterns","title":"Patterns","text":"<ul> <li>API Gateway - Single entry point</li> <li>Service Mesh - Infrastructure layer</li> <li>Circuit Breaker - Fault tolerance</li> <li>Bulkhead - Isolate failures</li> </ul> <p>Add more microservices topics as needed</p>"},{"location":"observability/","title":"Observability","text":"<p>Monitoring, logging, tracing, and metrics for distributed systems.</p>"},{"location":"observability/#overview","title":"Overview","text":"<p>Observability is the ability to understand the internal state of a system from its external outputs.</p>"},{"location":"observability/#three-pillars","title":"Three Pillars","text":""},{"location":"observability/#logs","title":"Logs","text":"<p>Structured logging for debugging and auditing.</p>"},{"location":"observability/#metrics","title":"Metrics","text":"<p>Quantitative measurements over time.</p>"},{"location":"observability/#traces","title":"Traces","text":"<p>Request flows through distributed systems.</p>"},{"location":"observability/#logging","title":"Logging","text":""},{"location":"observability/#structured-logging","title":"Structured Logging","text":"<pre><code>import logging\nimport json\n\nlogger = logging.getLogger(__name__)\n\nlogger.info(\n    \"Order created\",\n    extra={\n        \"order_id\": order_id,\n        \"user_id\": user_id,\n        \"amount\": amount\n    }\n)\n</code></pre>"},{"location":"observability/#log-levels","title":"Log Levels","text":"<ul> <li>DEBUG - Detailed information for debugging</li> <li>INFO - General informational messages</li> <li>WARNING - Warning messages</li> <li>ERROR - Error messages</li> <li>CRITICAL - Critical errors</li> </ul>"},{"location":"observability/#metrics_1","title":"Metrics","text":""},{"location":"observability/#types-of-metrics","title":"Types of Metrics","text":"<ul> <li>Counter - Incrementing values</li> <li>Gauge - Current value</li> <li>Histogram - Distribution of values</li> <li>Summary - Quantiles over time</li> </ul>"},{"location":"observability/#example","title":"Example","text":"<pre><code>from prometheus_client import Counter, Histogram\n\nrequest_count = Counter('requests_total', 'Total requests')\nrequest_duration = Histogram('request_duration_seconds', 'Request duration')\n</code></pre>"},{"location":"observability/#distributed-tracing","title":"Distributed Tracing","text":"<p>Track requests across service boundaries.</p>"},{"location":"observability/#opentelemetry","title":"OpenTelemetry","text":"<pre><code>from opentelemetry import trace\n\ntracer = trace.get_tracer(__name__)\n\nwith tracer.start_as_current_span(\"process_order\"):\n    # Your code here\n    pass\n</code></pre>"},{"location":"observability/#best-practices","title":"Best Practices","text":"<ul> <li>Use structured logging</li> <li>Include correlation IDs</li> <li>Set appropriate log levels</li> <li>Monitor key metrics</li> <li>Trace critical paths</li> <li>Set up alerts</li> </ul> <p>Add more observability topics as needed</p>"},{"location":"reliability/","title":"Reliability","text":"<p>Building reliable and resilient systems.</p>"},{"location":"reliability/#overview","title":"Overview","text":"<p>This section covers patterns and practices for building systems that can handle failures gracefully and maintain availability.</p>"},{"location":"reliability/#topics","title":"Topics","text":"<ul> <li>Fault tolerance patterns</li> <li>Circuit breakers</li> <li>Bulkheads</li> <li>Timeouts and retries</li> <li>Health checks</li> <li>Chaos engineering</li> </ul>"},{"location":"reliability/#reliability-patterns","title":"Reliability Patterns","text":""},{"location":"reliability/#circuit-breaker","title":"Circuit Breaker","text":"<p>Prevent cascading failures by stopping requests to failing services.</p> <pre><code>from circuitbreaker import circuit\n\n@circuit(failure_threshold=5, recovery_timeout=60)\ndef call_external_service():\n    # Your code here\n    pass\n</code></pre>"},{"location":"reliability/#retries-with-backoff","title":"Retries with Backoff","text":"<p>Retry failed operations with exponential backoff.</p>"},{"location":"reliability/#timeouts","title":"Timeouts","text":"<p>Set appropriate timeouts to prevent hanging requests.</p>"},{"location":"reliability/#health-checks","title":"Health Checks","text":"<p>Monitor service health and readiness.</p> <pre><code>@app.get(\"/health\")\ndef health_check():\n    return {\n        \"status\": \"healthy\",\n        \"checks\": {\n            \"database\": check_database(),\n            \"cache\": check_cache()\n        }\n    }\n</code></pre>"},{"location":"reliability/#availability-targets","title":"Availability Targets","text":"<ul> <li>99.9% (Three 9s) - ~8.76 hours downtime/year</li> <li>99.99% (Four 9s) - ~52.56 minutes downtime/year</li> <li>99.999% (Five 9s) - ~5.26 minutes downtime/year</li> </ul>"},{"location":"reliability/#failure-modes","title":"Failure Modes","text":"<ul> <li>Fail fast - Detect failures quickly</li> <li>Fail safe - Degrade gracefully</li> <li>Fail secure - Maintain security during failures</li> </ul>"},{"location":"reliability/#testing-reliability","title":"Testing Reliability","text":"<ul> <li>Chaos engineering - Test system resilience</li> <li>Load testing - Test under load</li> <li>Failure injection - Simulate failures</li> </ul> <p>Add more reliability topics as needed</p>"},{"location":"security/","title":"Security","text":"<p>Security best practices for backend systems.</p>"},{"location":"security/#overview","title":"Overview","text":"<p>This section covers security patterns, authentication, authorization, and secure coding practices.</p>"},{"location":"security/#topics","title":"Topics","text":"<ul> <li>Authentication and authorization</li> <li>API security</li> <li>Data encryption</li> <li>Secrets management</li> <li>Security headers</li> <li>OWASP Top 10</li> </ul>"},{"location":"security/#authentication","title":"Authentication","text":""},{"location":"security/#jwt-json-web-tokens","title":"JWT (JSON Web Tokens)","text":"<pre><code>import jwt\nfrom datetime import datetime, timedelta\n\ndef create_token(user_id):\n    payload = {\n        'user_id': user_id,\n        'exp': datetime.utcnow() + timedelta(hours=1)\n    }\n    return jwt.encode(payload, SECRET_KEY, algorithm='HS256')\n</code></pre>"},{"location":"security/#oauth2","title":"OAuth2","text":"<p>Standard protocol for authorization.</p>"},{"location":"security/#authorization","title":"Authorization","text":""},{"location":"security/#role-based-access-control-rbac","title":"Role-Based Access Control (RBAC)","text":"<p>Control access based on user roles.</p>"},{"location":"security/#attribute-based-access-control-abac","title":"Attribute-Based Access Control (ABAC)","text":"<p>Control access based on attributes.</p>"},{"location":"security/#api-security","title":"API Security","text":""},{"location":"security/#rate-limiting","title":"Rate Limiting","text":"<pre><code>from flask_limiter import Limiter\n\nlimiter = Limiter(\n    app,\n    key_func=get_remote_address,\n    default_limits=[\"100 per hour\"]\n)\n</code></pre>"},{"location":"security/#input-validation","title":"Input Validation","text":"<p>Always validate and sanitize input.</p>"},{"location":"security/#https","title":"HTTPS","text":"<p>Always use HTTPS in production.</p>"},{"location":"security/#secrets-management","title":"Secrets Management","text":"<ul> <li>Never commit secrets to version control</li> <li>Use environment variables or secret managers</li> <li>Rotate secrets regularly</li> </ul>"},{"location":"security/#security-headers","title":"Security Headers","text":"<pre><code>@app.after_request\ndef set_security_headers(response):\n    response.headers['X-Content-Type-Options'] = 'nosniff'\n    response.headers['X-Frame-Options'] = 'DENY'\n    response.headers['X-XSS-Protection'] = '1; mode=block'\n    response.headers['Strict-Transport-Security'] = 'max-age=31536000'\n    return response\n</code></pre>"},{"location":"security/#best-practices","title":"Best Practices","text":"<ul> <li>\u2705 Use HTTPS everywhere</li> <li>\u2705 Validate all input</li> <li>\u2705 Use parameterized queries</li> <li>\u2705 Implement rate limiting</li> <li>\u2705 Keep dependencies updated</li> <li>\u2705 Regular security audits</li> </ul> <p>Add more security topics as needed</p>"}]}